<html><head><title>Fit a Hidden Markov Model</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<link rel="stylesheet" type="text/css" href="Rchm.css">
</head>
<body>

<table width="100%"><tr><td>HMMFit(RHmm)</td><td align="right">R Documentation</td></tr></table><object type="application/x-oleobject" classid="clsid:1e2a7bd0-dab9-11d0-b93a-00c04fc99f9e">
<param name="keyword" value="R:   HMMFit">
<param name="keyword" value="R:   HMMFitClass">
<param name="keyword" value="R:   summary.HMMFitClass">
<param name="keyword" value="R:   print.summary.HMMFitClass">
<param name="keyword" value=" Fit a Hidden Markov Model">
</object>


<h2>Fit a Hidden Markov Model</h2>


<h3>Description</h3>

<p>
This function returns an HMMFitClass object which contains the results
of the Baum-Welch algorithm for the user's data
</p>


<h3>Usage</h3>

<pre>
    HMMFit(obs, dis="NORMAL", nStates=,  ...)
    HMMFit(obs, dis="DISCRETE", nStates=,  ...)
    HMMFit(obs, dis="MIXTURE", nStates=, nMixt=, ...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>obs</code></td>
<td>
A vector, a matrix, a data frame, a list of vectors or a list of matrices of observations. See section <B>obs parameter</B>.</td></tr>
<tr valign="top"><td><code>dis</code></td>
<td>
Distribution name = 'NORMAL', 'DISCRETE' or 'MIXTURE'. Default 'NORMAL'.</td></tr>
<tr valign="top"><td><code>nStates</code></td>
<td>
Number of hidden states. Default 2.</td></tr>
<tr valign="top"><td><code>nMixt</code></td>
<td>
Number of mixtures of normal distributions if dis ='MIXTURE'</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
optional parameter:
<dt>control</dt><dd>A list of control parameters for the Baum-Welch algorithm. See <B>control parameter</B></dd>
</td></tr>
</table>

<h3>Value</h3>

<p>
a HMMFitClass object:
</p>
<table summary="R argblock">
<tr valign="top"><td><code>HMM</code></td>
<td>
A HMMClass object with the fitted values of the model</td></tr>
<tr valign="top"><td><code>LLH</code></td>
<td>
log-likelihood</td></tr>
<tr valign="top"><td><code>BIC</code></td>
<td>
BIC criterium</td></tr>
<tr valign="top"><td><code>nIter</code></td>
<td>
Number of iterations of the Baum-Welch algorithm</td></tr>
<tr valign="top"><td><code>relVariation</code></td>
<td>
last relative variation of the LLH function</td></tr>
<tr valign="top"><td><code>asymptVar</code></td>
<td>
an HMMClass object with asymtotic variance of the parameters. See <B>asymptotic variance</B></td></tr>
<tr valign="top"><td><code>call</code></td>
<td>
The call object of the function call</td></tr>
</table>

<h3>obs parameter</h3>

<p>
If you fit the model with only one sample, obs is
either a vector (for univariate distributions) or a matrix (for multivariate distributions) or a data frame.
In the two last cases, the number of columns of obs defines the dimension of observations.<br><br>
</p>
<p>
If you fit the model with more than one sample, obs is a list of samples. Each element of obs is then a vector
(for univariate distributions) or a matrix (for multivariate distributions). The samples do not need to have the same length.<br><br>
</p>
<p>
For discrete distributions, obs can be a vector (or a list of vectors) of any type of R factor objects.
</p>


<h3>control parameter</h3>

<p>
<dt>init</dt><dd>Kind of initialisation ='KMEANS' (for univariate or multivariate normal distributions), 'RANDOM' or 'USER'. Default 'RANDOM', see <B>Random Initialization</B></dd>
<dt>iter</dt><dd>Maximum number of iterations for the Baum-Welch algorithm. Default 500</dd>
<dt>tol</dt><dd>Tolerance of the relative log-likehood augmentation. Default 1e-6</dd>
<dt>verbose</dt><dd>=0, no details, =1 iterations are displayed. Default 0</dd>
<dt>nInit</dt><dd>Number of random initialisations. Default 5</dd>
<dt>nIterInit</dt><dd>Number of maximum iterations of the Baum-Welch algorithm in the random initialisation phase. Default 5</dd>
<dt>initPoint</dt><dd>An HMMClass object used to initialize the parameters of the Baum-Welch algorithm. Default NULL.<br>
if initPoint != NULL, init is set to "USER"</dd>
</p>


<h3>Random initialization</h3>

<p>
'initProb' and 'transMat' parameters are uniformly drawn.<br><br>
For univariate normal distributions, empirical mean <i>m</i> and variance <i>s^2</i>
of all the samples are computed.
Then for every states,
an initial value of the 'mean' parameter is uniformly drawn between <i>m - 3s</i> and <i>m + 3s</i>
and an initial value of the 'var'
parameter is uniformly drawn between <i>0.5 s^2</i> and <i>3 s^2</i>.
<br>
For multivariate normal distributions, the same procedure is applied for each component of the mean vectors.
The initial covariance matrix is diagonal, and each initial variance is computed as for univariate models.
<br>
For mixtures of univariate normal distributions, initial values for 'mean' and 'var' parameters are computed
the same way than for normal distributions. The initial value of 'proportion' parameter is uniformly drawn.
<br>
For mixtures of multivariate normal distributions, the same procedure is applied for each component of the mean vectors, 
all the covariance matrices are diagonal and each initial variance is computed as for univariate models. The initial value 
of 'proportion' parameter is also uniformly drawn. 
<br>
For discrete distributions, the initial values of 'proba' parameters are uniformly drawn.
<br>
Of course, the initial values of the parameters 'initProba', 'proba', 'proportion' and 'transMat' are standardized to
ensure that they can represent probabilities vectors or transition matrices.
</p>


<h3>asymptotic variance</h3>

<p>
The asymptotic variance of estimates is computed using finite difference approximation.<br>
The summary and print.summary methods display the results.
</p>


<h3>References</h3>

<p>
Jeff A. Bilmes (1997) <EM> A Gentle Tutorial of the EM Algorithm and its Application to Parameter
Estimation for Gaussian Mixture and Hidden Markov Models</EM> <a href="http://ssli.ee.washington.edu/people/bilmes/mypapers/em.ps.gz">http://ssli.ee.washington.edu/people/bilmes/mypapers/em.ps.gz</a>
</p>
<p>
Ingmar Visser, Maartje E. J. Raijmakers and Peter C. M. Molenaar (2000) <EM>Confidence intervals for hidden Markov
model parameters</EM>, British Journal of Mathematical and Statistical Psychology, 53, 317-327.
</p>


<h3>Examples</h3>

<pre>
    data(geyser)
    obs &lt;- geyser$duration
    #Fits an 2 states gaussian model for geyser duration
    ResGeyser1 &lt;- HMMFit(obs)
    # fit a 3 states gaussian HMM for geyser duration
    # with iterations printing and kmeans initialization
    ResGeyser2 &lt;- HMMFit(obs, nStates=3, paramBW=list(verbose=1, init="KMEANS"))
    # fit a 2 states of a mixture of 3 normal distributions
    # for data_mixture
    data(data_mixture)
    ResMixture &lt;- HMMFit(data_mixture, nStates=2, nMixt=3, dis="MIXTURE")
    summary(ResMixture)
    # geyser data - 3 states HMM with bivariate normal distribution
    ResGeyser&lt;-HMMFit(obs=as.matrix(geyser), nStates=3)
    # multiple samples discrete observations
    data(weather)
    ResDiscrete &lt;- HMMFit(obs=weather, nStates=3, dis="DISCRETE")
</pre>



<hr><div align="center">[Package <em>RHmm</em> version 1.0.1 <a href="00Index.html">Index]</a></div>

</body></html>
